.default-cache:
  cache:
    key:
      files:
        - "poetry.lock"
        - ".pre-commit-config_template.yaml"
      prefix: dev-content
    paths:
      - $PIP_CACHE_DIR
      - $PRE_COMMIT_HOME
      - .venv/
      - node_modules/
      - .npm/
    policy: pull


.setup-network-certs: &setup-network-certs
  - section_start "Setup network certs" --collapsed
  - chmod 700 ${CERTIFICATE_SETUP_SCRIPT}
  - source ${CERTIFICATE_SETUP_SCRIPT}
  - section_end "Setup network certs"

.setup-artifactory: &setup-artifactory
  - section_start "Setup Artifactory" --collapsed
  - chmod 700 ${ARTIFACTORY_SETUP_SCRIPT}
  - source ${ARTIFACTORY_SETUP_SCRIPT}
  - section_end "Setup Artifactory"

### Global Script Snippets ###

.create-id-set:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json >> "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs/create_id_set.log"
  - cp ./Tests/id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}"
  - section_end "Create ID Set"

.create-id-set-xsoar:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "xsoar" >> "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs/create_id_set.log"
  - cp ./Tests/id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}"
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - section_end "Create ID Set"

.create-id-set-mp-v2:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "marketplacev2" >> "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs/create_id_set.log"
  - cp ./Tests/id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}"
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - section_end "Create ID Set"

.create-id-set-xpanse:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "xpanse" >> "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs/create_id_set.log"
  - cp ./Tests/id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}"
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - section_end "Create ID Set"

.clone-content-test-conf-and-infra:
  - section_start "Clone content-test-conf and infra" --collapsed
  - exec 3>&1 4>&2
  - exec > >(tee -a "${ARTIFACTS_FOLDER}/logs/clone_demisto_content_test_conf_and_infra.log") 2>&1
  - |
    set +e

    RED='\033[0;31m'
    GREEN='\033[0;32m'
    BLUE='\033[0;34m'
    YELLOW='\033[1;33m'
    PURPLE='\033[0;36m'
    NC='\033[0m'

    clone_repository() {
      local host=$1
      local user=$2
      local token=$3
      local repo_name=$4
      local branch=$5
      local retry_count=$6
      local sleep_time=${7:-10}  # default sleep time is 10 seconds.
      local exit_code=0
      local i=1
      echo -e "${PURPLE}Checking if repository repository:${repo_name}, project namespace:${CI_PROJECT_NAMESPACE} already exists in ${CI_PROJECT_DIR}/artifacts/repositories/${repo_name}${NC}"
      if [ -d "${CI_PROJECT_DIR}/artifacts/repositories/${repo_name}" ]; then
        echo -e "${GREEN}Repository ${repo_name} already exists, no need to clone it again${NC}"
        return 0
      fi
      echo -e "${PURPLE}Cloning repository:${repo_name}, project namespace:${CI_PROJECT_NAMESPACE} from ${host} branch:${branch} with ${retry_count} retries${NC}"
      if [ -z "${user}" ] && [ -z "${token}" ]; then
        user_info=""
      else
        user_info="${user}:${token}@"
        # If either user or token is not empty, then we need to add them to the url.
      fi
      pushd "${CI_PROJECT_DIR}/artifacts/repositories" || exit 1
      for ((i=1; i <= retry_count; i++)); do
        git -c advice.detachedHead=false clone --depth=1 "https://${user_info}${host}/${CI_PROJECT_NAMESPACE}/${repo_name}.git" --branch "${branch}" && exit_code=0 && break || exit_code=$?
        if [ ${i} -ne "${retry_count}" ]; then
          echo -e "${YELLOW}Failed to clone repository:${repo_name}, with branch:${branch}, project namespace:${CI_PROJECT_NAMESPACE}, exit code:${exit_code}, sleeping for ${sleep_time} seconds and trying again${NC}"
          sleep "${sleep_time}"
        else
          echo -e "${RED}Failed to clone repository:${repo_name} with branch:${branch}, project namespace:${CI_PROJECT_NAMESPACE}, exit code:${exit_code}, exhausted all ${retry_count} retries${NC}"
          break
        fi
      done
      popd || exit 1
      return ${exit_code}
    }

    clone_repository_with_fallback_branch() {
      local host=$1
      local user=$2
      local token=$3
      local repo_name=$4
      local branch=$5
      local retry_count=$6
      local sleep_time=${7:-10}  # default sleep time is 10 seconds.
      local fallback_branch="${8:-master}"

      # Check if branch exists in the repository.
      echo -e "${PURPLE}Checking if branch/tag ${branch} exists in repository:${repo_name}, project namespace:${CI_PROJECT_NAMESPACE}${NC}"
      if [ -z "${user}" ] && [ -z "${token}" ]; then
        user_info=""
      else
        # If either user or token is not empty, then we need to add them to the url.
        user_info="${user}:${token}@"
      fi
      local tag_exists
      local branch_type="branch"
      if [[ ${branch} =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
          echo "branch ${branch} is formatted like a version (v0.0.0), checking for a matching tag"
          git ls-remote --exit-code --quiet --tags "https://${user_info}${host}/${CI_PROJECT_NAMESPACE}/${repo_name}.git" "${branch}" 1>/dev/null 2>&1
          tag_exists=$?
          if [[ "${tag_exists}" -ne 0 ]]; then
            echo -e "${PURPLE}Could not find a tag called ${branch}${NC}"
          else
            branch_type="tag"
            echo -e "${GREEN}Found a tag called ${branch}${NC}"
          fi
      else
          echo "branch ${branch} is not formatted like a version (v0.0.0), skipping tag check"
          tag_exists=-1 # default, for when the branch isn't version-like named
      fi

      if [[ "${tag_exists}" -ne 0 ]]; then
          echo "Searching for a branch called ${branch}"
          git ls-remote --exit-code --quiet --heads "https://${user_info}${host}/${CI_PROJECT_NAMESPACE}/${repo_name}.git" "${branch}" 1>/dev/null 2>&1
          local branch_exists=$?
      fi

      if [[ "${tag_exists}" -ne 0 ]] && [[ "${branch_exists}" -ne 0 ]]; then
        echo -e "${YELLOW}branch/tag:${branch} does not exist in repository:${repo_name}, project namespace:${CI_PROJECT_NAMESPACE}, defaulting to ${fallback_branch}${NC}"
        local exit_code=1
      else
        echo -e "${GREEN}${branch_type}:${branch} exists in repository:${repo_name}, project namespace:${CI_PROJECT_NAMESPACE}, trying to clone${NC}"
        clone_repository "${host}" "${user}" "${token}" "${repo_name}" "${branch}" "${retry_count}" "${sleep_time}"
        local exit_code=$?
        if [ "${exit_code}" -ne 0 ]; then
          echo -e "${YELLOW}Failed to clone repository:${repo_name}, project namespace:${CI_PROJECT_NAMESPACE} with ${branch_type}:${branch}, exit code:${exit_code}${NC}"
        fi
      fi
      if [ "${exit_code}" -ne 0 ]; then
        # Trying to clone from fallback branch.
        echo -e "${PURPLE}Trying to clone repository:${repo_name}, project namespace:${CI_PROJECT_NAMESPACE} with fallback branch ${fallback_branch}!${NC}"
        clone_repository "${host}" "${user}" "${token}" "${repo_name}" "${fallback_branch}" "${retry_count}" "${sleep_time}"
        local exit_code=$?
        if [ ${exit_code} -ne 0 ]; then
          echo -e "${RED}ERROR: Failed to clone repository:${repo_name}, project namespace:${CI_PROJECT_NAMESPACE} with fallback branch:${fallback_branch}, exit code:${exit_code}, exiting!${NC}"
          exit ${exit_code}
        else
          echo -e "${GREEN}Successfully cloned repository:${repo_name}, project namespace:${CI_PROJECT_NAMESPACE} with fallback branch:${fallback_branch}${NC}"
          return 0
        fi
      else
        echo -e "${GREEN}Successfully cloned repository:${repo_name}, project namespace:${CI_PROJECT_NAMESPACE} with branch:${branch}${NC}"
        return 0
      fi
    }

    TEST_UPLOAD_BRANCH_SUFFIX="-upload_test_branch-"
    # Search for the branch name with the suffix of '-upload_test_branch-' in case it exists using CI_COMMIT_REF_NAME to clone content-test-conf.
    if [[ "${CI_COMMIT_REF_NAME}" == *"${TEST_UPLOAD_BRANCH_SUFFIX}"* ]]; then
      # Using bash string pattern matching to search only the last occurrence of the suffix, that's why we use a single '%'.
      SEARCHED_BRANCH_NAME_CONTENT_TEST_CONF="${CI_COMMIT_REF_NAME%"${TEST_UPLOAD_BRANCH_SUFFIX}"*}"
      echo "Found branch with suffix ${TEST_UPLOAD_BRANCH_SUFFIX} in branch name, using the branch ${SEARCHED_BRANCH_NAME_CONTENT_TEST_CONF} to clone content-test-conf repository"
    else
      # default to CI_COMMIT_REF_NAME when the suffix is not found.
      echo "Didn't find a branch with suffix ${TEST_UPLOAD_BRANCH_SUFFIX} in branch name, using the branch ${CI_COMMIT_REF_NAME} to clone content-test-conf repository, with fallback to master"
      SEARCHED_BRANCH_NAME_CONTENT_TEST_CONF="${CI_COMMIT_REF_NAME}"
    fi

    if [[ "${CURRENT_BRANCH_NAME}" == "master" ]] || [[ ${CURRENT_BRANCH_NAME} =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
      echo "Getting infra with branch:${CURRENT_BRANCH_NAME}"
      echo -e "${BLUE}NOTE: If you want to run the build on a specific branch in infra,\nYou need to replace the environment variable INFRA_BRANCH in the .gitlab-ci.yml file\nWith the name of the branch that exists in infra.${NC}"
    else
      echo "Getting infra with branch:${CURRENT_BRANCH_NAME}, with fallback to master"
    fi

    echo "Getting content-test-conf with branch${SEARCHED_BRANCH_NAME_CONTENT_TEST_CONF}, with fallback to master"

    SECRET_CONF_PATH="./conf_secret.json"
    echo ${SECRET_CONF_PATH} > secret_conf_path

    DEMISTO_LIC_PATH="./demisto.lic"
    echo ${DEMISTO_LIC_PATH} > demisto_lic_path

    DEMISTO_PACK_SIGNATURE_UTIL_PATH="./signDirectory"
    echo ${DEMISTO_PACK_SIGNATURE_UTIL_PATH} > demisto_pack_sig_util_path

    CI_SERVER_HOST=${CI_SERVER_HOST:-gitlab.xdr.pan.local} # disable-secrets-detection

    clone_repository_with_fallback_branch "${CI_SERVER_HOST}" "gitlab-ci-token" "${CI_JOB_TOKEN}" "content-test-conf" "${SEARCHED_BRANCH_NAME_CONTENT_TEST_CONF}" 3 10 "master"

    if [ ! -d "${CI_PROJECT_DIR}/config" ]; then mkdir -p "${CI_PROJECT_DIR}/config"; else echo "Directory ${CI_PROJECT_DIR}/config already exists"; fi
    cp -r "${CI_PROJECT_DIR}/artifacts/repositories/content-test-conf/config/"* "${CI_PROJECT_DIR}/config/"

    NAME_MAPPING_PATH="${CI_PROJECT_DIR}/config/name_mapping.json"
    if [ ! -f "${NAME_MAPPING_PATH}" ]; then
      echo "File ${NAME_MAPPING_PATH} does not exist, exiting!"
      exit 1;
    fi

    export CLOUD_SAAS_SERVERS_PATH="${CI_PROJECT_DIR}/config/saas_servers.json"
    if [ ! -f "${CLOUD_SAAS_SERVERS_PATH}" ]; then
      echo "File ${CLOUD_SAAS_SERVERS_PATH} does not exist, exiting!"
      exit 1
    fi

    echo "Cloning PrivatePacks"
    cp -r "${CI_PROJECT_DIR}/artifacts/repositories/content-test-conf/content/PrivatePacks/"* ./Packs
    echo "Cloned PrivatePacks"

    clone_repository_with_fallback_branch "${CI_SERVER_HOST}" "gitlab-ci-token" "${CI_JOB_TOKEN}" "infra" "${CURRENT_BRANCH_NAME}" 3 10 "master"


    cp -r "${CI_PROJECT_DIR}/artifacts/repositories/infra/demisto.lic" "${DEMISTO_LIC_PATH}"
    cp -r "${CI_PROJECT_DIR}/artifacts/repositories/infra/signDirectory" "${DEMISTO_PACK_SIGNATURE_UTIL_PATH}"

    [ ! -d "${CI_PROJECT_DIR}/Tests" ] && mkdir -p "${CI_PROJECT_DIR}/Tests" || echo "Tests directory already exists"
    [ ! -d "${CI_PROJECT_DIR}/Utils" ] && mkdir -p "${CI_PROJECT_DIR}/Utils" || echo "Utils directory already exists"
    [ ! -d "${CI_PROJECT_DIR}/config" ] && mkdir -p "${CI_PROJECT_DIR}/config" || echo "config directory already exists"
    [ ! -d "${CI_PROJECT_DIR}/SecretActions" ] && mkdir -p "${CI_PROJECT_DIR}/SecretActions" || echo "SecretActions directory already exists"
    [ ! -d "${CI_PROJECT_DIR}/gcp" ] && mkdir -p "${CI_PROJECT_DIR}/gcp" || echo "gcp directory already exists"
    # COPY DIRECTORIES
    cp -rf "${CI_PROJECT_DIR}/artifacts/repositories/infra/Tests/"* "${CI_PROJECT_DIR}/Tests"
    cp -rf "${CI_PROJECT_DIR}/artifacts/repositories/infra/Utils/"* "${CI_PROJECT_DIR}/Utils"
    cp -rf "${CI_PROJECT_DIR}/artifacts/repositories/infra/SecretActions/"* "${CI_PROJECT_DIR}/SecretActions"
    cp -rf "${CI_PROJECT_DIR}/artifacts/repositories/infra/gcp/"* "${CI_PROJECT_DIR}/gcp"
    # COPY FILES
    cp -f "${CI_PROJECT_DIR}/artifacts/repositories/infra/pyproject.toml"* "${CI_PROJECT_DIR}"
    cp -f "${CI_PROJECT_DIR}/artifacts/repositories/infra/poetry.lock"* "${CI_PROJECT_DIR}"

    set -e
    echo "Successfully cloned content-test-conf and infra repositories"

  - exec 1>&3 2>&4
  - exec 3>&- 4>&-
  - section_end "Clone content-test-conf and infra"

.create-artifacts-repositories-folder:
  - section_start "Create Artifacts Repositories Folder" --collapsed
  - |
    if [[ ! -d "${CI_PROJECT_DIR}/artifacts/repositories" ]]; then
      echo "Creating repositories folder:${CI_PROJECT_DIR}/artifacts/repositories"
      mkdir -p -m 777 "${CI_PROJECT_DIR}/artifacts/repositories" # using the -p to create the folder hierarchy.
    fi
  - section_end "Create Artifacts Repositories Folder"

.cloning-repositories:
  artifacts:
    expire_in: 30 days
    paths:
      - ${CI_PROJECT_DIR}/artifacts/*
      - ${CI_PROJECT_DIR}/pipeline_jobs_folder/*
    when: always
  before_script:
    - source .gitlab/helper_functions.sh
    - !reference [.setup-network-certs]
    - !reference [.create_artifacts_and_server_type_instance_folders]
    - !reference [.get_last_upload_commit]
    - !reference [.stop_contrib_external_build]
    - !reference [.create-artifacts-repositories-folder]
    - !reference [.clone-content-test-conf-and-infra]
  variables:
    ARTIFACTS_FOLDER: ${CI_PROJECT_DIR}/artifacts


.secrets-fetch:
  - section_start "Secrets Fetch" --collapsed
  - SECRET_CONF_PATH=$(cat secret_conf_path)
  - python3 ./SecretActions/SecretsBuild/add_secrets_file_to_build.py -sf "$SECRET_CONF_PATH" -u "$DEMISTO_USERNAME" -p "$DEMISTO_PASSWORD" --gsm_service_account "$GSM_SERVICE_ACCOUNT" --gsm_project_id_dev "$GSM_PROJECT_ID_DEV" --gsm_project_id_prod "$GSM_PROJECT_ID" --github_token "$GITHUB_TOKEN" >> "${ARTIFACTS_FOLDER}/logs/handle_secrets.log"
  - section_end "Secrets Fetch"

.check_build_files_are_up_to_date:
  - section_start "Check Build Files Are Up To Date"
  - |
    if [[ "${DEMISTO_SDK_NIGHTLY}" == "true" ]] || [[ "${IS_NIGHTLY}" == "true" ]] || [[ "${BUCKET_UPLOAD}" == "true" ]] || [[ -n "${SLACK_JOB}" ]] || [[ "${BUILD_MACHINES_CLEANUP}" == "true" ]] || [[ "${DELETE_MISMATCHED_BRANCHES}" == "true" ]] || [[ "${SECURITY_SCANS}" == "true" ]] || [[ "${DEMISTO_TEST_NATIVE_CANDIDATE}" == "true" ]] || [[ "${CI_COMMIT_BRANCH}" == "master" ]] || [[ "${SDK_RELEASE}" == "true" ]] || [[ "${TRIGGER_CONTRIBUTION_BUILD}" == "true" ]]; then
      echo "Running a build which doesn't require build files check validation"
    else
      git fetch origin master # allows `git diff`ing, otherwise there's no origin/master
      ./Tests/scripts/is_file_up_to_date.sh .gitlab $CI_COMMIT_BRANCH
      # we want to checkout if it's not up-to-date
      ./Tests/scripts/is_file_up_to_date.sh poetry.lock $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh pyproject.toml $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Config/core_packs_list.json $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Config/core_packs_mpv2_list.json $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Config/core_packs_xpanse_list.json $CI_COMMIT_BRANCH true
    fi
  - section_end "Check Build Files Are Up To Date"

.stop_contrib_external_build:
  - - section_start "Stop contrib external build"
  - |
    if [[ $CI_COMMIT_BRANCH =~ ^contrib/* && $CI_PIPELINE_SOURCE = "push" && $(curl --get --header "Accept: application/vnd.github.v3.raw" --header "Authorization: token $GITHUB_TOKEN" "https://api.github.com/repos/demisto/content/pulls?state=open&base=master" --data-urlencode "head=demisto:${CI_COMMIT_BRANCH}" | jq) = '[]' ]]; then
      echo "not running on contrib/ branches when there is no open internal PR or the pipeline was manually triggered"
      set -e
      exit 1
    fi
  - section_end "Stop contrib external build"

.create_artifacts_and_server_type_instance_folders: &create_artifacts_and_server_type_instance_folders
  - section_start "Create Artifacts, Server Instance, Server Type folders" --collapsed
  - |
    if [[ -n "${ARTIFACTS_FOLDER}" ]] && [[ ! -d "${ARTIFACTS_FOLDER}/logs" ]]; then
      echo "Creating Artifacts folder: ${ARTIFACTS_FOLDER} and it's log folder"
      mkdir -p -m 777 "${ARTIFACTS_FOLDER}/logs" # using the -p to create the logs folder as well.
    fi
  - |
    if [[ -n "${ARTIFACTS_FOLDER_INSTANCE}" ]] && [[ ! -d "${ARTIFACTS_FOLDER_INSTANCE}/logs" ]]; then
      echo "Creating Artifacts instance folder: ${ARTIFACTS_FOLDER_INSTANCE} and it's log folder"
      mkdir -p -m 777 "${ARTIFACTS_FOLDER_INSTANCE}/logs" # using the -p to create the logs folder as well.
      echo "${INSTANCE_ROLE}" > "${ARTIFACTS_FOLDER_INSTANCE}/instance_role.txt"
    fi
  - |
    if [[ -n "${ARTIFACTS_FOLDER_SERVER_TYPE}" ]] && [[ ! -d "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs" ]]; then
      echo "Creating Artifacts Server type folder: ${ARTIFACTS_FOLDER_SERVER_TYPE} and it's log folder"
      mkdir -p -m 777 "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs" # using the -p to create the logs folder as well.
      echo "${SERVER_TYPE}" > "${ARTIFACTS_FOLDER_SERVER_TYPE}/server_type.txt"
    fi
  - section_end "Create Artifacts, Server Instance, Server Type folders"

.clone_and_export_variables: &clone_and_export_variables
  - section_start "Git - Job Start Actions" --collapsed
  - git fetch origin master:refs/remotes/origin/master
  - git checkout -B $CI_COMMIT_BRANCH $CI_COMMIT_SHA
  - git config diff.renameLimit 6000
  - section_end "Git - Job Start Actions"
  - section_start "Source BASH Environment" --collapsed
  - |
    if [[ -f "$BASH_ENV" ]]; then
      source "$BASH_ENV"
    fi
  - source Utils/content_release_vars.sh
  # DEMISTO_SDK_GRAPH_FORCE_CREATE set to true to create graph from scratch.
  - |
    if [[ "${IS_NIGHTLY}" == "true" ]]; then
      echo "set DEMISTO_SDK_GRAPH_FORCE_CREATE to true to create graph from scratch"
      export DEMISTO_SDK_GRAPH_FORCE_CREATE=true
      echo "DEMISTO_SDK_GRAPH_FORCE_CREATE was set to true to create graph from scratch"
      echo $DEMISTO_SDK_GRAPH_FORCE_CREATE
    fi
  - section_end "Source BASH Environment"
  - section_start "Granting execute permissions on files" --collapsed
  - chmod +x ./Tests/scripts/*
  - chmod +x ./Tests/Marketplace/*
  - chmod +x ./SecretActions/SecretsBuild/*
  - section_end "Granting execute permissions on files"

.get_contribution_pack: &get_contribution_pack
  - section_start "getting contrib packs" --collapsed
  - |
    if [[ -n "${CONTRIB_BRANCH}" ]]; then
      USERNAME=$(echo $CONTRIB_BRANCH | cut -d ":" -f 1)
      BRANCH=$(echo $CONTRIB_BRANCH | cut -d ":" -f 2)
      python3 ./Utils/update_contribution_pack_in_base_branch.py -p $PULL_REQUEST_NUMBER -b $BRANCH -u $USERNAME -c $CONTRIB_REPO -gt $GITHUB_TOKEN
    fi
  - section_end "getting contrib packs"

.install_venv: &install_venv
  - section_start "Installing Virtualenv" --collapsed
  # we still need to install even if cached. if cached, `poetry` will handle it
  - echo "installing venv, with always copy:${POETRY_VIRTUALENVS_OPTIONS_ALWAYS_COPY}"
  - NO_HOOKS=1 .hooks/bootstrap | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
  - echo "Checking if pyproject.toml is consistent with poetry.lock"
  - poetry lock --check
  - npm ci --cache .npm --prefer-offline | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
  - source ./.venv/bin/activate
  - |
    if [[ "${DEMISTO_SDK_NIGHTLY}" == "true" || "${OVERRIDE_SDK_REF}" == "true" ]]; then
      echo "Installing SDK from ${SDK_REF}" | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
      pip3 uninstall -y demisto-sdk | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
      pip3 install "git+https://github.com/demisto/demisto-sdk@${SDK_REF}#egg=demisto-sdk" | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
    else
      echo "Using SDK from pyproject.toml" | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
    fi
  - |
      python3 --version | tee -a "${ARTIFACTS_FOLDER}/logs/installed_python_libraries.log"
      python3 -m pip list | tee -a "${ARTIFACTS_FOLDER}/logs/installed_python_libraries.log"
  - section_end "Installing Virtualenv"

.ssh-config-setup:
  - section_start "SSH config setup" --collapsed
  - cp $GCP_SSH_CONFIGURATION ~/.ssh/config
  - chmod 700 ~/.ssh/config
  - section_end "SSH config setup"

.install_ssh_keys: &install_ssh_keys
  - section_start "Installing SSH keys" --collapsed
  - eval $(ssh-agent -s)
  - chmod 400 $OREGON_CI_KEY
  - ssh-add $OREGON_CI_KEY
  - mkdir -p ~/.ssh
  - chmod 700 ~/.ssh
  - section_end "Installing SSH keys"

.install_node_modules: &install_node_modules
  - section_start "Installing node modules" --collapsed
  - EXIT_CODE=0
  - source $NVM_DIR/nvm.sh
  - nvm use default | tee --append "${ARTIFACTS_FOLDER}/logs/installations_node.log"
  - echo "Installing Node Modules" | tee --append "${ARTIFACTS_FOLDER}/logs/installations_node.log"
  - npm ci --cache .npm --prefer-offline >> "${ARTIFACTS_FOLDER}/logs/installations_node.log" 2>&1
  - npm list --json >> "${ARTIFACTS_FOLDER}/logs/installations_node.log" 2>&1
  - npm link jsdoc-to-markdown@5.0.3  >> "${ARTIFACTS_FOLDER}/logs/installations_node.log" 2>&1 || EXIT_CODE=$?  # disable-secrets-detection
  - if [ ${EXIT_CODE} -ne 0 ]; then
      cp /root/.npm/_logs/*.log "${ARTIFACTS_FOLDER}/logs/";
      exit ${EXIT_CODE};
    fi
  - section_end "Installing node modules"

.get_last_upload_commit: &get_last_upload_commit
  - section_start "Getting last bucket upload commit" --collapsed
  - gsutil cp "gs://$GCS_PRODUCTION_BUCKET/content/packs/index.json" "${ARTIFACTS_FOLDER_SERVER_TYPE}/previous_index.json"
  - export LAST_UPLOAD_COMMIT=$(cat "${ARTIFACTS_FOLDER_SERVER_TYPE}/previous_index.json" | jq -r ".\"commit\"")
  - section_end "Getting last bucket upload commit"

.neo4j-setup: &neo4j-setup
  - section_start "Neo4j Setup" --collapsed
  - neo4j-admin dbms set-initial-password contentgraph
  - neo4j start
  - section_end "Neo4j Setup"

.build_parameters: &build_parameters
  - section_start "Build Parameters" --collapsed
  - echo "Environment Variables:"
  - set | grep -E "^ARTIFACTS_FOLDER.*=|^JIRA_.*=|^INSTANCE_TESTS=|^SERVER_BRANCH_NAME=|^ARTIFACT_BUILD_NUM=|^IS_NIGHTLY=|^TIME_TO_LIVE=|^CONTRIB_BRANCH=|^FORCE_PACK_UPLOAD=|^PACKS_TO_UPLOAD=|^BUCKET_UPLOAD=|^TEST_UPLOAD=|^FORCE_BUCKET_UPLOAD=|^STORAGE_BASE_PATH=|^OVERRIDE_ALL_PACKS=|^GCS_MARKET_BUCKET=|^GCS_MARKET_V2_BUCKET=|^GCS_MARKET_XPANSE_BUCKET=|^SLACK_.*=|^NVM_DIR=|^NODE_VERSION=|^PATH=|^ENV_RESULTS_PATH=|^LAST_UPLOAD_COMMIT=|^DEMISTO_SDK_LOG_FILE_SIZE=|^DEMISTO_SDK_LOG_FILE_COUNT=|^DEMISTO_SDK_LOG_FILE_PATH=|^DEMISTO_SDK_LOG_NO_COLORS=|^DEMISTO_SDK_LOG_NOTIFY_PATH=|^POETRY_VIRTUALENVS_OPTIONS_ALWAYS_COPY=|^DEMISTO_SDK_NIGHTLY=|^OVERRIDE_SDK_REF=|^SDK_REF=|^CURRENT_BRANCH_NAME=" | sort
  - echo "Versions Installed:"
  - python --version
  - python3 --version
  - poetry --version
  - pip3 --version
  - node --version
  - npm --version
  - jsdoc2md --version
  - demisto-sdk --version || true
  - section_end "Build Parameters"

.gitlab_ci_build_parameters: &gitlab_ci_build_parameters
  - section_start "Gitlab CI Build Parameters" --collapsed
  - set | grep -E "^CI_.*=|^GITLAB.*=" | sort
  - section_end "Gitlab CI Build Parameters"

.checkout-upload-commit-content-nightly: &checkout-upload-commit-content-nightly
  - section_start "Checkout upload commit content nightly" --collapsed
  - |
    if [[ "${IS_NIGHTLY}" == "true" && "${CI_COMMIT_BRANCH}" == "master" && "${OLD_NIGHTLY}" == "true" ]]; then
      if [[ ! -d "${CI_PROJECT_DIR}/artifacts/production_packs" ]]; then
        echo -e "${RED}ERROR: content production packs do not exist in ${CI_PROJECT_DIR}/artifacts${NC}"
        exit 1
      fi
      rm -rf ./Packs
      echo "copying production Packs folder from ${CI_PROJECT_DIR}/artifacts/production_packs to ./Packs"
      cp -r ${CI_PROJECT_DIR}/artifacts/production_packs ./Packs
      git config core.fileMode false  # used to tell git not to identify permission changes on files as changes
      echo "the Packs changes between upload commit ${LAST_UPLOAD_COMMIT} to master commit ${CI_COMMIT_SHA} is:"
      git status -- Packs  # show the differences between the upload commit to the master branch for the Packs folder
      echo "The Packs folder is in the state of commit $LAST_UPLOAD_COMMIT"
    else
      echo "not checking out to the latest upload commit $LAST_UPLOAD_COMMIT because the build is not content nightly"
    fi
  - section_end "Checkout upload commit content nightly"

.extract_content_test_conf: &extract_content_test_conf
  - section_start "extract_content_test_conf" --collapsed
  - |
    if [[ "${IS_NIGHTLY}" == "true" || "${EXTRACT_PRIVATE_TESTDATA}" == "true" ]]; then
        python ./Tests/scripts/extract_content_test_conf.py --content-path "${CI_PROJECT_DIR}" --content-test-conf-path "${CI_PROJECT_DIR}/artifacts/repositories/content-test-conf" --missing-content-packs-test-conf "${ARTIFACTS_FOLDER_SERVER_TYPE}/missing_content_packs_test_conf.txt"
    fi
  - section_end "extract_content_test_conf"

.export_cloud_machine_constants:
  # exporting the machine credentials

  - IFS=', ' read -r -a CLOUD_CHOSEN_MACHINE_ID_ARRAY <<< "${CLOUD_CHOSEN_MACHINE_IDS}"
  - |
    for CLOUD_CHOSEN_MACHINE_ID in "${CLOUD_CHOSEN_MACHINE_ID_ARRAY[@]}"; do
      export XSIAM_SERVER_CONFIG=$(jq -r ".[\"${CLOUD_CHOSEN_MACHINE_ID}\"]" < "${CLOUD_SAAS_SERVERS_PATH}")
      export XSIAM_MACHINE_DETAILS=$(python Tests/scripts/get_cloud_machines_details.py --cloud_machine_ids "${CLOUD_CHOSEN_MACHINE_ID}" | jq -r ".[\"${CLOUD_CHOSEN_MACHINE_ID}\"]")

      export DEMISTO_BASE_URL=$(echo "$XSIAM_SERVER_CONFIG" | jq -r ".[\"base_url\"]")
      export XSIAM_AUTH_ID=$(echo "$XSIAM_MACHINE_DETAILS" | jq -r ".[\"x-xdr-auth-id\"]")
      export DEMISTO_API_KEY=$(echo "$XSIAM_MACHINE_DETAILS" | jq -r ".[\"api-key\"]")
      export XSIAM_TOKEN=$(echo "$XSIAM_MACHINE_DETAILS" | jq -r ".[\"token\"]")
      break
    done

.default-before-script:
  before_script:
    - source .gitlab/helper_functions.sh
    - *setup-network-certs
    - *setup-artifactory
    - *gitlab_ci_build_parameters
    - *create_artifacts_and_server_type_instance_folders
    - !reference [.create-artifacts-repositories-folder]
    - !reference [.clone-content-test-conf-and-infra]
    - *clone_and_export_variables
    - *install_node_modules
    - *install_venv
    - *get_contribution_pack
    - *get_last_upload_commit
    - *install_ssh_keys
    - *neo4j-setup
    - *build_parameters
    - *checkout-upload-commit-content-nightly
    - *extract_content_test_conf

.default-after-script:
  - source .gitlab/helper_functions.sh
  - *setup-network-certs
  - *setup-artifactory
  - *install_node_modules
  - *install_venv

.add-content-production-to-artifacts:
  - section_start "Clone production content and add it to artifacts" --collapsed
  - |
      if [[ "${IS_NIGHTLY}" == "true" && "${CI_COMMIT_BRANCH}" == "master" && "${OLD_NIGHTLY}" == "true" ]]; then
        echo "Cloning production content"
        mkdir content_production
        cd content_production
        git init > /dev/null 2>&1
        git remote add origin https://gitlab-ci-token:${CI_JOB_TOKEN}@${CI_SERVER_HOST}/${CI_PROJECT_NAMESPACE}/content.git
        git fetch --depth 1 origin $LAST_UPLOAD_COMMIT
        git checkout FETCH_HEAD >${ARTIFACTS_FOLDER}/logs/add-content-production-to-artifacts.log 2>&1
        cp -r ./Packs ${ARTIFACTS_FOLDER}/production_packs
        echo "checked out ${LAST_UPLOAD_COMMIT} which is the last successful upload commit"
      else
        echo "not cloning production content because the build is not content nightly"
      fi
  - section_end "Clone production content and add it to artifacts"

.default-job-settings:
  interruptible: true
  extends:
    - .default-cache
    - .default-before-script
  needs:
    - job: cloning-content-repo-last-upload-commit
      optional: true

.trigger-slack-notification:
  stage: .post
  trigger:
    include:
      - file: .gitlab/ci/content-ci/ci/.gitlab-ci.slack-notify.yml
        ref: $INFRA_BRANCH
        project: "${CI_PROJECT_NAMESPACE}/infra"
  inherit: # see https://gitlab.com/gitlab-org/gitlab-runner/-/issues/27775
    variables: false

.destroy_xsoar_instances:
  - section_start "Destroy Instances"
  - python3 ./Tests/scripts/destroy_instances.py --artifacts-dir "${ARTIFACTS_FOLDER}" --env-file "${ENV_RESULTS_PATH}" --instance-role "${INSTANCE_ROLE}"
  - destroy_instances_exit_code=$?
  - |
    if [ "${destroy_instances_exit_code}" -ne 0 ]; then
      echo "Failed to destroy instances of role ${INSTANCE_ROLE}, exit code: ${destroy_instances_exit_code}"
    fi
  - section_end "Destroy Instances"

.lock-machine:
  - section_start "Lock Machine" --collapsed
  - ./Tests/scripts/lock_cloud_machines.sh
  - export CLOUD_CHOSEN_MACHINE_IDS=$(cat "${ARTIFACTS_FOLDER}/locked_machines_list.txt")
  - echo "CLOUD Chosen machine ids are:${CLOUD_CHOSEN_MACHINE_IDS}"
  - section_end "Lock Machine"

.unlock-machine:
  - section_start "Unlock Machine" --collapsed
  - |
    if [[ -f "${ARTIFACTS_FOLDER}/locked_machines_list.txt" ]]; then
      export CLOUD_CHOSEN_MACHINE_IDS=$(cat "${ARTIFACTS_FOLDER}/locked_machines_list.txt")

      if [[ -n "${CLOUD_CHOSEN_MACHINE_IDS}" ]]; then
        if [[ -e "${ARTIFACTS_FOLDER}/locked_machines_list.txt" ]]; then
          echo "Job finished, removing lock file for machine ids:${CLOUD_CHOSEN_MACHINE_IDS}"
          gcloud auth activate-service-account --key-file="$GCS_ARTIFACTS_KEY" >> "${ARTIFACTS_FOLDER}/logs/gcloud_auth.log" 2>&1
          gsutil rm "gs://xsoar-ci-artifacts/$GCS_LOCKS_PATH/machines_locks/*-lock-$CI_PIPELINE_ID*" || true
          echo "Finished removing lock file(s)"
        else
          echo "No lock file found, skipping unlocking"
        fi
      else
        echo "No machine ids were chosen, skipping unlocking"
      fi
    else
      echo "No locked_machines_list.txt file found, skipping unlocking"
    fi
  - section_end "Unlock Machine"

.clean-machine:
  #retry mechanism for trigger upload pipeline in case it failed because of gitlab connectivity issues.
  - section_start "Trigger Cleanup Machine" --collapsed
  - echo "needed clean machine ${CLEAN_MACHINE_NEEDED}"
  - |
    if [[ -f "${ARTIFACTS_FOLDER}/locked_machines_list.txt" ]]; then
      export CLOUD_CHOSEN_MACHINE_IDS=$(cat "${ARTIFACTS_FOLDER}/locked_machines_list.txt")
    fi
  - |
    if [[ "${CLEAN_MACHINE_NEEDED}" == "true" ]] && [[ -e "${ARTIFACTS_FOLDER}/locked_machines_list.txt" ]]; then
      echo "machine ids:${CLOUD_CHOSEN_MACHINE_IDS}"
      #trigger pipeline for each machine
      IFS=', ' read -r -a CLOUD_CHOSEN_MACHINE_ID_ARRAY <<< "${CLOUD_CHOSEN_MACHINE_IDS}"
      for CLOUD_CHOSEN_MACHINE_ID in "${CLOUD_CHOSEN_MACHINE_ID_ARRAY[@]}"; do
        echo  "Cleanup - ${CLOUD_CHOSEN_MACHINE_ID} machine"
        # retry mechanism for trigger upload pipeline in case it failed because of gitlab connectivity issues.
        for _ in {1..3}; do
          export pipeline_id=$(./Utils/gitlab_triggers/trigger_cleanup_build_machines.sh -b "${CURRENT_BRANCH_NAME}" -ct "${TRIGGER_INFRA_TOKEN}" -m "${CLOUD_CHOSEN_MACHINE_ID}" -mc "${CLOUD_MACHINES_COUNT}" -mt "${CLOUD_MACHINES_TYPE}" -p "${GCS_LOCKS_PATH}" -st "${SERVER_TYPE}" -op "${CI_PIPELINE_ID}" | jq .id)
          if [ "${pipeline_id}" != "null" ]; then
            echo "Triggered Clean Machine pipeline id:${pipeline_id} server type:${SERVER_TYPE}"
            echo "rename lock file of ${CLOUD_CHOSEN_MACHINE_ID}, jobid: ${CI_JOB_ID} ,current pipeline id: ${CI_PIPELINE_ID} to new pipeline_id ${pipeline_id}"
            gcloud auth activate-service-account --key-file="$GCS_ARTIFACTS_KEY" >> "${ARTIFACTS_FOLDER}/logs/gcloud_auth.log" 2>&1
            gsutil mv "gs://xsoar-ci-artifacts/${GCS_LOCKS_PATH}/machines_locks/${CI_PROJECT_ID}-lock-${CLOUD_CHOSEN_MACHINE_ID}-lock-${CI_PIPELINE_ID}_${CI_JOB_ID}" "gs://xsoar-ci-artifacts/${GCS_LOCKS_PATH}/machines_locks/${INFRA_PROJECT_ID}-lock-${CLOUD_CHOSEN_MACHINE_ID}-lock-${pipeline_id}"
            echo "Successful triggered cleanup machine pipeline - ${CI_SERVER_URL}/${CI_PROJECT_NAMESPACE}/infra/-/pipelines/$pipeline_id"
            break
          fi
          echo "Sleeping for 10 seconds before retrying"
          sleep 10
        done
      done
    else
      echo "No need to clean machine"
    fi
  - section_end "Trigger Cleanup Machine"


.split-packs-to-machines:
  - section_start "Split packs to chosen machines" --collapsed
  - IFS=', ' read -r -a CLOUD_CHOSEN_MACHINE_ID_ARRAY <<< "${CLOUD_CHOSEN_MACHINE_IDS}"
  - echo "There are ${#CLOUD_CHOSEN_MACHINE_ID_ARRAY[@]} chosen cloud machines."
  - python3 Tests/scripts/split_packs_to_machines.py --cloud_machines "${CLOUD_CHOSEN_MACHINE_IDS}" --server_type "${SERVER_TYPE}" --service_account "${GCS_ARTIFACTS_KEY}" --artifacts-path "${ARTIFACTS_FOLDER_SERVER_TYPE}" --marketplace "${MARKETPLACE_NAME}"
  - section_end "Split packs to chosen machines"

.install_packs_by_machines:
  - !reference [.split-packs-to-machines]
  - section_start "Install Packs and run Test-Module"
  - ./Tests/scripts/install_content_and_test_integrations.sh || EXIT_CODE=$?
  - if [ "${EXIT_CODE}" -ne 0 ]; then exit 1; fi
  - cp -f "${ARTIFACTS_FOLDER_SERVER_TYPE}/conf.json" Tests/conf.json
  - section_end "Install Packs and run Test-Module"


.cloud-machine-information:
  - section_start "Cloud Machine information"
  - ./Tests/scripts/print_cloud_machine_details.sh
  - section_end "Cloud Machine information"

.uninstall-packs-and-reset-bucket-cloud:
  - section_start "Uninstall Packs and Reset Bucket Cloud" --collapsed
  - ./Tests/scripts/uninstall_packs_and_reset_bucket_cloud.sh || EXIT_CODE=$?
  - section_end "Uninstall Packs and Reset Bucket Cloud"

.update-content-graph:
  - section_start "Update content graph" --collapsed
  - echo "Updating the content graph"
  - demisto-sdk graph update -g --marketplace "${MARKETPLACE_NAME}" -i "${ARTIFACTS_FOLDER_SERVER_TYPE}/content_graph/${MARKETPLACE_NAME}.zip"
  - echo "Successfully updated content graph"
  - section_end "Update content graph"

.validate_content_test_conf_branch_merged:
  - section_start "Validate content-test-conf Branch Merged"
  - |
    if [[ "${CI_COMMIT_BRANCH}" = "master" ]]; then
      echo "Skipping, Should not run on master branch."
    elif [ 'true' = $(./Tests/scripts/check_if_branch_exist.sh -u "gitlab-ci-token" -t "${CI_JOB_TOKEN}" -h "${CI_SERVER_HOST}" --repo "${CI_PROJECT_NAMESPACE}/content-test-conf" -b "${CI_COMMIT_BRANCH}") ]; then
      RED='\033[0;31m'
      NC='\033[0m'
      echo -e "${RED}ERROR: Found a branch with the same name:${CI_COMMIT_BRANCH} in contest-test-conf repository.\n Merge it in order to merge the current branch into content repo.${NC}"
      job-done
      exit 1
    else
      echo "Couldn't find a branch with the name:${CI_COMMIT_BRANCH} in contest-test-conf repository."
    fi
  - section_end "Validate content-test-conf Branch Merged"

.pre-commit-settings:
  tags:
    - gce
  stage: unittests-and-validations
  artifacts:
    reports:
      junit: .pre-commit/pytest-junit/*.xml
      coverage_report:
        coverage_format: cobertura
        path: ${CI_PROJECT_DIR}/artifacts/coverage_report/coverage.xml
    expire_in: 30 days
    paths:
      - ${CI_PROJECT_DIR}/unit-tests
      - ${CI_PROJECT_DIR}/.pre-commit/pytest-junit
      - ${CI_PROJECT_DIR}/artifacts/*
      - ${CI_PROJECT_DIR}/pipeline_jobs_folder/*
    when: always
  variables:
    DOCKER_HOST: tcp://docker:2375
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_CERTDIR: ""
  extends:
    - .default-job-settings
    - .docker_services

.test-infrastructure:
  stage: unittests-and-validations
  extends:
    - .default-job-settings
  script:
    - section_start "Test Infrastructure"
    - ./Tests/scripts/sdk_pylint_check.sh
    - section_end "Test Infrastructure"
    - job-done

.run-pre-commit:
  cache:
    policy: pull-push
  extends:
    - .pre-commit-settings
  variables:
    KUBERNETES_MEMORY_REQUEST: 16Gi
    KUBERNETES_MEMORY_LIMIT: 16Gi
  script:
    - section_start "Configure Docker" --collapsed
    # we need to configure the docker with the registry in order to be able to pull the images
    - gcloud auth configure-docker ${DOCKER_IO_DOMAIN} >> "${ARTIFACTS_FOLDER}/logs/configure_docker_with_registry.log" 2>&1
    - section_end "Configure Docker"
    - section_start "Run Pre-Commit"
    - SHOULD_PRE_COMMIT_ALL=$(./Tests/scripts/should_pre_commit_all.sh)
    - echo "Using PRE_COMMIT_DOCKER_IMAGE:${PRE_COMMIT_DOCKER_IMAGE}"
    # SHOULD_PRE_COMMIT_ALL is a string and therefore might come back empty from should_pre_commit_all.sh
    - |
      if [[ -n "${SHOULD_PRE_COMMIT_ALL}" ]]; then
        echo "${SHOULD_PRE_COMMIT_ALL}" >> "${ARTIFACTS_FOLDER}/logs/should_pre_commit_all.log"
        echo "SHOULD_PRE_COMMIT_ALL is set, view the reason in the logs: ${ARTIFACTS_FOLDER}/logs/should_pre_commit_all.log"
      else
        echo "SHOULD_PRE_COMMIT_ALL isn't set"
      fi
    - poetry install --only mypy
    - echo "in .run-pre-commit, SHOULD_PRE_COMMIT_ALL = $SHOULD_PRE_COMMIT_ALL"
    - PRE_COMMIT_SUCCESS=0
    - |
      if [[ "${BUCKET_UPLOAD}" == "true" ]] && [[ "${TEST_UPLOAD}" == "true" ]]; then
        echo "Skipping validations when uploading to a test bucket."
      else
        if [[ "${BUCKET_UPLOAD}" == "true" ]]; then
          demisto-sdk pre-commit -g --prev-version $LAST_UPLOAD_COMMIT --mode=nightly --docker-image="${PRE_COMMIT_DOCKER_IMAGE}" || PRE_COMMIT_SUCCESS=1
        else
          if [[ -n "${SHOULD_PRE_COMMIT_ALL}" ]]; then
            echo "Pre Commit all files"
            # if we need to pre-commit all anyway we need the graph, and it's better (resource-wise) to create it here.
            demisto-sdk graph update
            unset DEMISTO_SDK_GRAPH_FORCE_CREATE  # The graph is already up, no need to force create it
            demisto-sdk pre-commit -a --mode=nightly --docker-image="${PRE_COMMIT_DOCKER_IMAGE}" || PRE_COMMIT_SUCCESS=1
          elif [[ "$CI_COMMIT_BRANCH" =~ ^AUD-demisto/.* ]]; then
            echo "In docker auto update branch, Pre Commit only changed files with relevant hooks."
            demisto-sdk pre-commit --mode=docker_autoupdate --docker-image="${PRE_COMMIT_DOCKER_IMAGE}" || PRE_COMMIT_SUCCESS=1
          else
            echo "Pre Commit only changed files"
            demisto-sdk pre-commit --mode=ci --docker-image="${PRE_COMMIT_DOCKER_IMAGE}" || PRE_COMMIT_SUCCESS=1
          fi
        fi
      fi
    - |
      if [[ -d coverage_report ]]; then
        cp -r coverage_report artifacts/coverage_report
      fi
    - | # we upload the coverage report to the GCS only for the pre-commit "from-yml" job on nightly runs
      if [[ "${IS_NIGHTLY}" == "true" && "$CI_COMMIT_BRANCH" == "master" && "${PRE_COMMIT_DOCKER_IMAGE}" == "from-yml" ]]; then
        python3 Utils/upload_code_coverage_report.py --service_account $GCS_MARKET_KEY --source_file_name "${ARTIFACTS_FOLDER}/coverage_report/coverage.json" --minimal_file_name "${ARTIFACTS_FOLDER}/coverage_report/coverage-min.json"
      fi
    - |
      pre_commit_result_dir="${ARTIFACTS_FOLDER}/pre-commit/${PRE_COMMIT_DOCKER_IMAGE}"
      if [ ! -d "${pre_commit_result_dir}" ]; then
        mkdir -p "${pre_commit_result_dir}"
      else
        echo "Directory ${pre_commit_result_dir} already exists"
      fi
      cp "${CI_PROJECT_DIR}/.pre-commit-config_template.yaml" "${pre_commit_result_dir}/"
    - echo "PRE_COMMIT_SUCCESS=$PRE_COMMIT_SUCCESS"
    - section_end "Run Pre-Commit"
    - job-done
    - exit "${PRE_COMMIT_SUCCESS}"

.run-validations:
  stage: unittests-and-validations
  tags:
    - gce
  extends:
    - .default-job-settings
  variables:
    KUBERNETES_MEMORY_REQUEST: 16Gi
    KUBERNETES_MEMORY_LIMIT: 16Gi
  artifacts:
    expire_in: 30 days
    paths:
      - ${CI_PROJECT_DIR}/artifacts/*
      - ${CI_PROJECT_DIR}/pipeline_jobs_folder/*
    when: always
  script:
    - section_start "Copy conf.json To Server Type Artifacts Folder"
    - cp "./Tests/conf.json" "${ARTIFACTS_FOLDER_SERVER_TYPE}/conf.json"
    - section_end "Copy conf.json To Server Type Artifacts Folder"
    - section_start "Validate Files and Yaml"
    - |
      if [[ "$CI_COMMIT_BRANCH" =~ ^AUD-demisto/.* ]]; then
        echo "In auto update docker flow, skipping validate and linter execution."
      else
        ./Tests/scripts/validate.sh
      fi
    - section_start "Validate Files and Yaml"
    - section_start "Check Spelling"
    - python3 ./Tests/scripts/circleci_spell_checker.py $CI_COMMIT_BRANCH
    - section_end "Check Spelling"
    - section_start "Validate landingPageSections.json"
    - echo "Download index.zip"
    - INDEX_PATH=$(mktemp)
    - |
      gcloud auth activate-service-account --key-file="$GCS_MARKET_KEY" >> "${ARTIFACTS_FOLDER}/logs/gcloud_auth.log" 2>&1
      echo "successfully activated google cloud service account"
      gsutil cp "gs://marketplace-dist/content/packs/index.zip" $INDEX_PATH
      echo "successfully downloaded index.zip"
    - echo "successfully downloaded index.zip into $INDEX_PATH"

    - UNZIP_PATH=$(mktemp -d)
    - unzip $INDEX_PATH -d $UNZIP_PATH > "${ARTIFACTS_FOLDER}/logs/unzip_index.log"

    - python3 Tests/Marketplace/validate_landing_page_sections.py -i $UNZIP_PATH
    - section_end "Validate landingPageSections.json"
    - section_start "Revoking GCP Auth"
    - gcloud auth revoke "${GCS_ARTIFACTS_ACCOUNT_NAME}" >> "${ARTIFACTS_FOLDER}/logs/gcloud_auth.log" 2>&1
    - section_end "Revoking GCP Auth"
    - job-done

.run-validations-new-validate-flow:
  stage: unittests-and-validations
  extends:
    - .default-job-settings
  tags:
    - gce
  variables:
    KUBERNETES_MEMORY_REQUEST: 16Gi
    KUBERNETES_MEMORY_LIMIT: 16Gi
  artifacts:
    expire_in: 30 days
    paths:
      - ${CI_PROJECT_DIR}/artifacts/*
      - ${CI_PROJECT_DIR}/pipeline_jobs_folder/*
    when: always
  script:
    - section_start "Validate Files and Yaml"
    - |
      if [[ "$CI_COMMIT_BRANCH" =~ ^AUD-demisto/.* ]]; then
        echo "In auto update docker flow, skipping validate and linter execution."
      else
        ./Tests/scripts/new_validate.sh
      fi
    - section_end "Validate Files and Yaml"
    - job-done

.jobs-done-check:
  stage: are-jobs-really-done
  extends:
    - .default-job-settings
  script:
    - python3 Tests/scripts/check_jobs_done.py --triggering-workflow "${WORKFLOW}" --job-done-files "${PIPELINE_JOBS_FOLDER}"

.docker_services:
  services:
    - name: ${DOCKER_IO}/library/docker:20.10.12-dind
      alias: docker
      variables:
        KUBERNETES_MEMORY_REQUEST: ${KUBERNETES_MEMORY_REQUEST}
        KUBERNETES_MEMORY_LIMIT: ${KUBERNETES_MEMORY_LIMIT}
